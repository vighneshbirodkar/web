date tue jan gmt server ncsa last modified tue sep gmt content type text html content length reinforcement learning information access real learning problem information access rich sutton university massachusetts rich cs umass edu presented aaai stanford spring symposium machine learning information access march many thanks rik belew jude shavlik introductory patter talk try take new look learning problem information access structured training information really available likely available delays decision making receipt relevant feedback newcomer information access experience reinforcement learning one main lessons reinforcement learning really important understand true nature learning problem want solve example illustrates whole idea gerry tesauro ibm built world best computer player backgammon neural network trained examples human expert moves tried reinforcement learning approach trained network expert examples simply playing observing outcomes month self play program became new world champ computers extremely strong player par world best grandmasters learning self play approach worked well primarily could generate new training data expert trained network always limited examples laboriously constructed human experts self play training data may individually less informative much generated cheaply big win long run may true information access right use training sets documents labeled experts relevant relevant training data always expensive scarce small much better would could generate kind training data online normal use system data may imperfect unclear certainly plentiful may also truer important sense expert labeled training sets artificial accurately mirror real usage backgammon expert trained system could learn mimic experts win game online trained system able learn play better experts training data real challenge think information access uncover real structure learning problem learning done online learning thrives data data data get data need online normal operation system without relying expensive expert labeled training sets talk proceeds three parts first introduction reinforcement learning second examines parts learning problem information access like solved reinforcement learning methods information access problem doesn map exactly onto reinforcement learning problem special structure third part talk examine special structure kind new learning methods might applied rest approximations slides presented talk conclusions advance learning ia information access like learning everywhere never told right answers sequential problem actions affect opportunities reinforcement learning addresses issues learning powerful done online normal operation online data feedback like ia reinforcement learning learning trial error rewards punishments active multidisciplinary research area overall approach ai based learning interaction environment integrates learning planning reacting handles stochastic uncertain environments recent large scale world class applications particular learning mechanisms learning less helpful feedback classical machine learning supervised learning situation action correct action situation action correct action correct action supplied objective correct actual actions effect interaction independent self contained reinforcement learning situation action reward situation action reward situation action agent never told action correct agent told nothing actions selected actions may affect next situation object maximize future rewards harder problem real problem problems relevance feedback documents shown exploration exploitation dilemma degrees relevance want make user happy short term many solutions require sequences steps support early steps sl used reliably line except immed prediction learn normal operation applications rl td gammon jellyfish tesauro dahl elevator control crites job shop scheduling zhang dietterich mobile robot controllers lin miller thrun computer vision peng et al natural language dialog tuning gorin henis characters interactive games handelman lane airline seat allocation hutchinson manufacturing composite materials sofge white key ideas rl algorithms value functions like heuristic state evaluation function learned approximates expected future reward state action idea learn good action rather whether best taking account long term affects value functions vastly simplify everything td methods efficient way learning predict e g value functions experience search learning guess guess large space rl algorithms major components rl agent policy reward good value good predicts reward model follows info access applications rl anytime decisions made desired choice immediately clear anytime want make long term predictions classical ir querying routing filtering rl situation query user model documents actions present document rankings reward user feedback presented docs pro rl feedback selective exactly fit sl framework con rl feedback exactly fit rl framework problem sequential e g bartell cottrell belew boyan freitag joachim sch tze hull pederson multistep info access problems query search optimization entertainment software ir agents information assistant routing filtering interface manager web browsing anticipating user sense learning complex interactive goal directed input sensitive sequence steps exactly rl good multi step sequential nature ia web page led web page request user enabled much better query query whose results enabled user refine next query ordering search steps document turned useful series searches building prior results imagine ideal info access system continuous oportunity provide query info keywords type specs feedback continuously updated list proposed documents find good ones soon possible actions things could done pursue search send queries alta vista yahoo ask user synonyms types utilities documents propose links follow else consult situations whole current status search reward good bad buttons maintaining interest etc value good action rewards lead shortcutting feedback often good bad often indicate desired response one situation whole sequence prior situations good document positive example looking negative example wasn found earlier result search generalized learned anticipated shortcutted anticipation process similar certain rl processes compare classical context large numbers documents e g million queries e g way queries used learn docs web large numbers documents even queries always readings writings thus learn docs good good keywords appropriate popularity ratings priors documents q decide access today scientific papers books movies web pages recommendations reviewed journals movie critics cool site day visitors site colleagues talking hard find good stuff web classical ir concept good stuff docs relevant good bad differences similarities users users provide feedback favor help others paid program forces ought providing feedback selfish reasons suppose personal research assistant wouldn tell liked didn like user differences selfish feedback known user similarities summary data power relevant data available relevance vs utility independent vs multi step queries shortcutting collaborative filtering selfish feedback learning classifications help