date tue nov gmt server ncsa content type text html last modified thu oct gmt content length cs processes part iii implementation cs lecture notesprocesses synchronization part iii implementation processes contents implementing monitors implementing semaphores implementing critical sections short term scheduling implementing processes presented processes user point view bottom starting process concept introducing semaphores way synchronizing processes finally adding higher level synchronization facility form monitors explain implement things opposite order starting monitors finishing mechanism making processes run tanenbaum makes big deal showing various synchronization primitives equivalent section true kind misses point easy implement semaphores monitors saw first part project way usually works normally semaphores something like implemented using lower level facilities used implement monitors implementing monitors assuming semaphores would rather monitors assume semaphores extra operation beyond standards operations semaphore awaited returns true processes currently waiting semaphore feature normally provided semaphores race condition limits usefulness time awaited returns true process may done making false happens problem way use semaphores implement monitors since monitors language feature implemented help compiler response keywords monitor condition signal wait notify compiler inserts little bits code program worry compiler manages concern code works monitor keyword says mutual exclusion methods monitor class effect similar making every method synchronized method java thus compiler creates semaphore mutex initialized adds muxtex head method also adds chunk code call exit described place method may return end procedure return statement point exception may thrown place goto might leave procedure language gotos etc finding return points tricky complicated procedures want compiler help us process signals notifies condition variable process waiting problem processes immediately continue since would violate cardinal rule may never one process active methods monitor object time thus must block one processes signaller case signal waiter case notify call semaphore highpriority since processes blocked given preference processes blocked mutex trying get form outside highpriority semaphore initialized zero condition variable c becomes semaphore c sem initialized zero c wait becomes highpriority awaited highpriority else mutex c sem process blocks condition variable lets process go ahead preferably one waiting highpriority semaphore operation c signal becomes c sem awaited c sem highpriority notice signal condition awaited effect signal condition awaited immediately blocks signaller finally code exit placed every return point highpriority awaited highpriority else mutex note code c wait except final c sem systems use notify java c notify replaced c sem awaited c sem systems code c wait also modified wait highpriority semaphore getting semaphore associated condition highpriority awaited highpriority else mutex c sem highpriority system offers signal notify generic implementation monitors optimized special cases first note process exits monitor immediately signal need wait highpriority semaphore turns common occurrence worth optimizing special case signal allowed return implementation simplified see fig page tanenbaum finally note use full generality semaphores implementation monitors semaphore mutex takes values called binary semaphore semaphores never value zero implementing semaphores simple minded attempt implement semaphores might look like class semaphore private int value semaphore int v value v public void value value public void value two things wrong solution first seen attempts manipulate shared variable without synchronization lead incorrect results even manipulation simple value monitors could make modifications value atomic making class monitor making method synchronized remember monitors implemented semaphores implement semaphores something even primitive assume critical sections bracket section code begin cs end cs begin cs something end cs code execute atomically protected semaphore mutex something mutex mutex semaphore initialized course actually use semaphore implement semaphores show implement begin cs end cs next section problem implementation semaphores includes busy wait semaphore waiting value become non zero looping continuously testing value even waiting process running cpu busy waiting may slow processes since repeatedly accessing shared memory thus interfering accesses memory cpu shared memory unit respond one cpu time one cpu problem even worse process calling running another process wants call may get chance run need way put process sleep semaphores could use semaphore need something primitive let us assume data structure called pcb short process control block contains information process procedure swap process takes pointer pcb argument swap process pcb called state currently running process one called swap process saved pcb cpu starts running process whose state previously stored pcb instead given begin cs end cs swap process complete implementation semaphores quite simple subtle class semaphore private pcb queue waiters processes waiting semaphore private int value negative number waiters static pcb queue ready list list processes ready run semaphore int v value v public void begin cs value value current process must wait find process run ready list must non empty global deadlock pcb pcb ready list dequeue swap process pcb pcb contains state process called currently running process process waiters enqueue pcb end cs public void begin cs value value value previously negative process waiting must wake pcb pcb waiters dequeue ready list enqueue pcb end cs semaphore implementation swap process magic procedure probably really written assembly language describe java assume cpu current stack pointer register accessible sp void swap process pcb pcb int new sp pcb saved sp pcb saved sp sp sp new sp mentioned earlier process stack stack frame procedure process called yet completed stack frame contains least enough information implement return procedure address instruction called procedure pointer caller stack frame cpu devotes one registers call sp point current stack frame process currently running cpu encounters return statement reloads sp pc program counter registers stack frame approximate description pseudo java might something like class stackframe int callers sp int callers pc stackframe sp current stack pointer return instruction address return point sp callers pc sp sp callers sp goto return point course isn really goto statement java would done hardware sequence assembly language statements suppose process p calls swap process pcb pcb saved sp points stack frame representing call swap process process p call swap process creates frame p stack makes sp point second statement swap process saves pointer stack frame pcb third statement loads sp pointer p stack frame swap process procedure returns return whatever procedure called swap process process p implementing critical sections final piece puzzle implement begin cs end cs several ways depending hardware configuration first suppose multiple cpu accessing single shared memory unit generally memory bus hardware serializes requests read write memory words example two cpu try write different values memory word time net result one two values combination values similarly one cpu tries read memory word time another modifies read return either old new value see half changed memory location surprisingly hardware support need implement critical sections first solution problem discovered dutch mathematician dekker simpler solution later discovered gary peterson peterson solution looks deceptively simple see tricky problem let us look couple simpler incorrect solutions assume two processes p p first idea processes take turns shared int turn depending void begin cs int process version begin cs turn nothing void end cs int process version end cs turn give process chance solution certainly safe never allows processes critical sections time problem solution live process p wants enter critical section turn wait process p decides enter leave critical section since used critical sections protect short operations see implementation semaphores reasonable assume process done begin cs soon end cs converse true reason assume process want enter critical section time near future even get around problem second attempt solve problem uses shared array critical indicate processes critical sections shared boolean critical false false void begin cs int critical true critical nothing void end cs int critical false solution unfortunately prone deadlock processes set critical flags true time loop forever waiting process go ahead switch order statements begin cs solution becomes unsafe processes could check critical states time see false enter critical sections finally change code void begin cs int critical true critical critical false perhaps sleep critical true livelock occur processes get loop process sets critical flag notices critical flag true clears critical flag repeats peterson correct solution combines ideas attempts like second solution process signals desire enter critical section setting shared flag like first solution uses turn variable uses break ties shared int turn shared boolean critical false false void begin cs int critical true let guy know m trying turn nice let go first critical j guy trying turn precedence nothing void end cs int critical false m done peterson solution correct drawbacks first employs busy wait sometimes called spin lock bad reasons suggested however critical sections used protect short sections code operations semaphores isn bad problem two processes rarely attempt enter critical sections time even loser spin brief time serious problem peterson solution works two processes next present three solutions work arbitrary numbers processes computers additional hardware features make critical section easier solve one feature test set instruction sets memory location given value time records cpu unshared state information location previous value example old value might loaded register condition code might set indicate whether old value zero tanenbaum presents fig page solution using test set version using java like syntax shared boolean lock false true process cs void begin cs processes boolean key testandset lock key return void end cs lock false computers swap instruction swaps value register contents shared memory word shared boolean lock false true process cs void begin cs processes boolean key true swap key lock key return void end cs boolean key false swap key lock problem solutions necessarily prevent starvation several processes try enter critical sections time one succeed safety winner chosen bounded amount time liveness winner chosen essentially randomly nothing prevent one process winning time bakery algorithm leslie lamport solves problem process wants get service takes ticket process lowest numbered ticket served first process id used break ties static final int n number processes shared boolean choosing false false false shared int ticket void begin cs int choosing true ticket max ticket ticket n choosing false int j j n j choosing j nothing ticket j ticket j ticket ticket j ticket j nothing void end cs int ticket finally note solutions critical section problem assume multiple cpu sharing one memory one cpu cannot afford busy wait however good news make sure short term scheduler discussed next section switch processes process critical section one way simply block interrupts computers way preventing interrupts occurring dangerous block interrupts extended period time fine short critical sections ones used implement semaphores note process blocks semaphore need mutual exclusion whole time blocked critical section long enough decide whether block short term scheduling earlier called process blocked runnable said runnable process either ready running general list runnable processes called ready list cpu picks process ready list runs blocks chooses another process run implementation semaphores illustrates switching among runnable processes called short term scheduling algorithm decides process run long run called short term scheduling policy discipline policies preemptive meaning cpu may switch processes even current process isn blocked look various scheduling policies worthwhile think trying accomplish tension maximizing overall efficiency giving good service individual customers system point view two important measures throughput amount useful work accomplished per unit time depends course constitutes useful work one common measure throughput jobs minute second hour depending kinds job utilization device utilization device fraction time device busy good scheduling algorithm keeps devices cpu disk drives etc busy time measures depend scheduling algorithm also offered load load light jobs arrive infrequently throughput utilization low however good scheduling algorithm throughput increase linearly load available hardware saturated throughput levels job also wants good service general good service means good response starts quickly runs quickly finishes quickly several ways measuring response turnaround length time job arrives system finally finishes response time length time job arrives system starts produce output interactive jobs response time might important turnaround waiting time amount time job ready runnable running better measure scheduling quality turnaround since scheduler control amount time process spends computing blocked waiting o penalty ratio elapsed time divided sum cpu o demands job still better measure well scheduler measures many times worse turnaround would ideal system job never wait another job could allocate o device soon wants experienced overhead operating system functions would penalty ratio takes twice long complete would perfect system penalty ration measure overall performance combine performance jobs using one measures way combining example compute average waiting time average waiting times jobs similarly could calculate sum waiting times average penalty ratio variance response time etc evidence high variance response time annoying interactive users high mean within reason since concentrating short term cpu scheduling one useful way look process sequence bursts burst computation done process time becomes ready next time blocks short term scheduler burst looks like tiny job first come first served simplest possible scheduling discipline called first come first served fcfs ready list simple queue first first scheduler simply runs first job queue blocks runs new first job job becomes ready simply added end queue example use illustrate scheduling disciplines burst arrival time burst length b c d e times milliseconds following gantt chart shows schedule results fcfs scheduling main advantage fcfs easy write understand severe problems one process gets infinite loop run forever shut others even assume processes infinite loops take special precautions catch processes fcfs tends excessively favor long bursts let compute waiting time penalty ratios jobs burst start time finish time waiting time penalty ratio b c d e average see shorted burst c worst penalty ratio situation much worse short burst arrives long one example suppose burst length arrives time burst length arrives immediately time first burst doesn wait penalty ratio perfect second burst waits milliseconds penalty ratio favoring long bursts means favoring cpu bound processes long cpu bursts o operations general would like favor o bound processes since give cpu o bound process quickly finish burst start o get ready list consider happens one cpu bound process several o bound processes suppose start right foot run o bound processes first quickly finish bursts go start o operations leaving us run cpu bound job finish o queue behind cpu bound job leaving o devices idle cpu bound job finishes burst start o operation allowing us run jobs quickly finish bursts start o cpu sitting idle processes o since cpu hog started o first likely finish first grabbing cpu making processes wait system continue way alternating periods cpu busy o devices idle periods cpu idle processes o destroyed one main motivations processes first place allow overlap computation o phenomenon called convoy effect summary although fcfs simple performs poorly terms global performance measures cpu utilization throughput also gives lousy response interactive jobs tend o bound one good thing fcfs starvation every burst get served waits long enough shortest job first much better policy called shortest job first sjf whenever cpu choose burst run chooses shortest one algorithm really called shortest burst first name sjf traditional policy certainly gets around problems fcfs mentioned fact prove sjf optimal respect average waiting time policy whatsoever worse average waiting time decreasing average waiting time also improve processor utilization throughput proof sjf optimal suppose set bursts ready run run order sjf must burst run shorter burst say b run b b b reversed order would increase waiting time b b decrease waiting time b b since b b net decrease total hence average waiting time continuing manner move shorter bursts ahead longer ones eventually end bursts sorted increasing order size think bubble sort previous example sjf scheduling burst start time finish time waiting time penalty ratio b c d e average gantt chart described sjf non preemptive policy also preemptive version sjf sometimes called shortest remaining time first srtf whenever new job enters ready queue algorithm reconsiders job run new arrival burst shorter remaining portion current burst scheduler moves current job back ready queue appropriate position considering remaining time burst runs new arrival instead sjf srtf starvation possible long burst may never get run shorter bursts keep arriving ready queue return problem later one problem sjf srtf know long burst going run luckily make pretty good guess processes tend creatures habit one burst process long good chance next burst long well thus might guess burst length previous burst process however strategy won work well process occasional oddball burst unusually long short burst get burst wrong guess wrong next burst typical process better idea make guess average length immediately preceding burst guess used burst guess guess previous burst strategy takes account entire past history process guessing next burst length quickly adapts changes behavior process since weight burst computing guess drops exponentially time since burst call recent burst length b one b etc next guess b b b b round robin processor sharing another scheme preventing long bursts getting much priority preemptive strategy called round robin rr rr keeps burst queue runs first one like fcfs length time q called quantum current burst hasn completed moved tail queue next burst started gantt charts example round robin quantum sizes q get average waiting time average penalty ratio work q averages drop respectively limit q approaches zero called processor sharing ps ps causes cpu shared equally among ready processes steady state ps bursts enter leave ready list burst sees penalty ratio exactly n length ready queue course ps theoretical interest substantial overhead switching one process another quantum small cpu spend time switching processes practically none actually running priority scheduling whole family scheduling algorithms use priorities basic idea always run highest priority burst priority algorithms preemptive non preemptive burst arrives higher priority currently running burst switch immediately wait current burst finishes priorities assigned externally processes based importance also assigned changed dynamically example priorities used prevent starvation raise priority burst longer ready queue eventually highest priority ready burst guaranteed chance finish one interesting use priority sometimes called multi level feedback queues mlfq maintain sequence fifo queues numbered starting zero new bursts added tail queue always run burst head lowest numbered non empty queue doesn complete complete within specified time limit moved tail next higher queue queue time limit one unit queue two units queue four units queue eight units queue etc scheme combines many best features algorithms favors short bursts since completed still low numbered high priority queues long bursts hand run comparatively expensive process switches idea generalized queue scheduling discipline use criterion like move bursts queue queue end number algorithms dream analysis possible analyze algorithms mathematically whole branch computer science called queuing theory concerned sort analysis usually analysis uses statistical assumptions example common assume arrival new bursts poisson expected time wait next new burst arrives independent long since last burst arrived words amount time passed since last arrival clue long next arrival show case probability arrival next milliseconds e parameter called arrival rate average time arrivals another common assumption burst lengths follow similar exponential distribution probability length burst less e bt b another parameter service rate average burst length b kind system called m m queue ratio p b particular interest p burst arriving average faster finishing ready queue grows without bound course happen one burst per process theory p arrivals departures perfectly balanced shown fcfs average penalty ratio bursts length p p p bt see decreases penalty ratio increases proving fcfs doesn like short bursts also note p approaches one penalty ration approaches infinity processor sharing noticed processes penalty ratio length queue shown average length p see medium term long term scheduling later course job might batch job printing run paychecks interactive login session command issued interactive session might consist single process group related processes actually b p supposed greek letters alpha beta rho figure make html solomon cs wisc edu thu oct cst copyright marvin solomon rights reserved