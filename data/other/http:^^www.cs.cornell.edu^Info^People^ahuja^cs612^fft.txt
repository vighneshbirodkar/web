mime version server cern date sunday dec gmt content type text html content length last modified monday may gmt cs final report parallel ffts overview ffts currently calculated phase cycles communication computation number cycles equal number dimensions problem communication phase distributed matrix transpose implemented personalized black box function dimensional fft would need cycles phases d fft distributed transpose d fft distributed transpose data initially distributed processor n xm rowsxcolumns block matrix e column block distribution n n number rows columns problem size m m n p n p respectively p number processors process computes fft local data calls distributed transpose function results processor matrix size n xm fft matrix computed followed another distributed transpose phase problems communication phase motivations implementing distributed transpose operation black box inefficient due unused processor cycles waiting sending data idle floating point units sends recieves function starts collecting data processors data stored linearly sent directly processors nth m rows m columns goes nth processor collecting done trivially block copying column time matrix stored column major b done processor sends signals processors indicating ready recieve data processors proceeds send data processors recieves ready signal processors waits incoming data arrive b transpose local buffer b recieving end recieving end processor wastes processor cycles waiting data processors arrive transpose buffer synchronization wait also wastes fpus sp flow control load store operations data network interface memory since require integer arithmetic b sending end sending end suffers lot overhead message sent greater k length see section thrust break one big message number messages smaller k eliminating synchronization waits try eliminate synchronization waits performing operations small blocks exchanged processors thereby shifting stance collective communication point point communication requires modification initial data distribution lead efficient implementation computation changes column cyclic distributed matrix transpose phase changes collecting block processor performing last part computation cycle block sending processor distribution column cyclic data collected rows stride p processor recieves block transposes performs first part computation cycle applied block scatters data stride p across rows loop unrolled processors waste time waiting data every time processor polls network incoming packet gets data needs operate see figure forms waiting eliminated need wait blocks since operate block independently need poll network block since unroll loop schedule communication get packet every time poll network processor got data perform main body computation limitations find cost copying data message packets prohibitive sas data gathered sending scattered recieving stride p element wise copy data inefficient limiting packet size implementation active messages sp sends messages k chunks messages greater k wait acknowledgement recieving processor next chunk sent therefore suffer entire roundtrip latency network call store async therefore return till chunks sent avoid overhead send packets size k store async return immediately processor get next packet ready sending allows parallelism send operation k message collected packet size m xm collectd matrix described previous section row stride r column stride r r m number rows k packet r m number columns k packet collecting packet last part computation cycle applied sending recieving end k packet transposed first part computation cycle applied followed scatter operation m xm buffer column stride r elements column copied contiguously done number times till m xm buffer received data till sending side finished sending m xm buffer note previous case loop unrolled processor sending data processors recieving data processors avoid waiting packets network limitations find cost copying buffers prohibhitive attaining good efficiency note copying case copying earlier case collect data stride p main array m xm buffer previous case copy data k packets row column stride r r respectively sent network recieving end k packet copied m xm buffer column stride r packets arrived m xm buffer merged main buffer contiguous copy columns lot copying element wise except copying k packet m xm buffer m xm buffer main array done column wise therefore cannot take advantage block copying functions results conclusions future work seen approach work due copying computation overhead introduces next step look overheads removed