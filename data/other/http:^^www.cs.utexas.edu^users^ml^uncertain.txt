mime version server cern date tuesday jan gmt content type text html content length last modified wednesday aug gmt uncertain reasoning uncertain reasoning view paper click open book image combining symbolic connectionist learning methods refine certainty factor rule bases j jeffrey mahoney ph d thesis department computer sciences university texas austin may research describes system rapture designed revise rule bases expressed certainty factor format recent studies shown learning facilitated biased domain specific expertise also shown many real world domains require form probabilistic uncertain reasoning order successfully represent target concepts rapture designed take advantage results beginning set certainty factor rules along accurately labelled training examples rapture makes use symbolic connectionist learning techniques revising rules order correctly classify training examples modified version backpropagation used adjust certainty factors rules id information gain heuristic used add new rules upstart algorithm used create new hidden terms rule base results refining four real world rule bases presented demonstrate effectiveness combined approach two rule bases designed identify particular areas strands dna one identifying infectious diseases fourth attempts diagnose soybean diseases results rapture compared backpropagation c kbann learning systems rapture generally produces sets rules accurate systems often creating smaller sets rules using less training time refinement bayesian networks combining connectionist symbolic techniques sowmya ramanchandran ph d proposal department computer sciences university texas austin bayesian networks provide mathematically sound formalism representing reasoning uncertain knowledge widely used however acquiring capturing knowledge framework difficult growing interest formulating techniques learning bayesian networks inductively problem learning bayesian network given complete data explored depth problem learning networks unobserved causes still open proposal view problem perspective theory revision present novel approach adapts techniques developed revising theories symbolic connectionist representations thus assume learner given initial approximate network usually obtained expert technique inductively revises network fit data better proposed system two components one component revises parameters bayesian network known structure component revises structure network component parameter revision maps given bayesian network multi layer feedforward neural network parameters mapped weights neural network uses standard backpropagation techniques learn weights structure revision component uses qualitative analysis suggest revisions network fails predict data accurately first component implemented present results experiments real world classification problems show technique effective also discuss proposed structure revision algorithm plans experiments evaluate system well extensions system revising bayesian network parameters using backpropagation sowmya ramachandran raymond j mooney proceedings international conference neural networks icnn special session knowledge based artificial neural networks washington dc june problem learning bayesian networks hidden variables known hard problem even simpler task learning conditional probabilities bayesian network hidden variables hard paper present approach learns conditional probabilities bayesian network hidden variables transforming multi layer feedforward neural network ann conditional probabilities mapped onto weights ann learned using standard backpropagation techniques avoid problem exponentially large anns focus bayesian networks noisy noisy nodes experiments real world classification problems demonstrate effectiveness technique comparing methods refining certainty factor rule bases j jeffrey mahoney raymond j mooney proceedings eleventh international workshop machine learning pp rutgers nj july ml paper compares two methods refining uncertain knowledge bases using propositional certainty factor rules first method implemented rapture system employs neural network training refine certainties existing rules uses symbolic technique add new rules second method based one used kbann system initially adds complete set potential new rules low certainty allows neural network training filter adjust rules experimental results indicate former method results significantly faster training produces much simpler refined rule bases slightly greater accuracy modifying network architectures certainty factor rule base revision j jeffrey mahoney raymond j mooney proceedings international symposium integrating knowledge neural heuristics pp pensacola fl may isiknh paper describes rapture system revising probabilistic rule bases converts symbolic rules connectionist network trained via connectionist techniques uses modified version backpropagation refine certainty factors rule base uses id information gain heuristic quinlan add new rules work currently way finding improved techniques modifying network architectures include adding hidden units using upstart algorithm frean case made via comparison fully connected connectionist techniques keeping rule base close original possible adding new input units needed combining connectionist symbolic learning refine certainty factor rule bases j jeffrey mahoney raymond j mooney connection science pp special issue architectures integrating neural symbolic processing paper describes rapture system revising probabilistic knowledge bases combines connectionist symbolic learning methods rapture uses modified version backpropagation refine certainty factors mycin style rule base uses id information gain heuristic add new rules results refining three actual expert knowledge bases demonstrate combined approach generally performs better previous methods combining symbolic neural learning revise probabilistic theories j jeffrey mahoney raymond j mooney proceedings machine learning workshop integrated learning real domains aberdeen scotland july paper describes rapture system revising probabilistic theories combines symbolic neural network learning methods rapture uses modified version backpropagation refine certainty factors mycin style rule base uses id information gain heuristic add new rules results two real world domains demonstrate combined approach performs well better previous methods estlin cs utexas edu