mime version server cern date monday dec gmt content type text html content length last modified saturday mar gmt homework homework handed tue mar th due thu mar th find one partner term project select projet place page proposal want web proposal must address following points title page project description goals plan attack expected kind results determine sucess failure critical steps get done soon create subdir vol www info courses current cs projects place proposal name file welcome html ideas could lead interesting projects short term self contained projects could easily accomplished within context term project others could eventually expand publications meng project pick problem wider scope must sure isolate piece adequately addressed within semester may work teams two possibly three clearly identifiable role member team especially three person teams new high bandwidth cu seeme couple split c u net split c solarismp build atm fast ethernet gateway using u net ip level u net level implement u net kernel endpoints run standard ip u net add ip packet filter sba firmware fast ethernet implement active messages flow control shared fast ethernet implement port parallel gdb split c splash benchmarks perfect fast rpc u net implementation urpc last year perfect distributed shared memory last year usit instruction set design measurement condition codes question suggested david wood architectures designed employ condition codes several recent risc architectures yet others example mips architecture condition codes yet set instructions put result comparison general purpose register rs virtual condition code registers set either side effect alu instruction explicitly compare instructions explore similarities differences two schemes one clearly better differences technology dependent processors become increasingly integrated scheme lead best performance analyze instruction issue strategies analyze compare scoreboarding tomasulo algorithm rs scheme beef much advantage dynamic scheduling achieved static scheduling delay slots trade offs change increased processor memory speed ratio lock free caches write buffers critical pull together good tools students built berkeley years ago didn much time left study results though could leverage work address spaces beyond bits according observers demand virtual address space increases rate bit every years thus bit architectures quite acceptable throughout eventually became constraining almost entirely replaced rate years bit addresses begin constrain programs architectures introduced segments extend address space computer architects suggest full bit machines integers addresses solve problems explore cost performance programming compatibility issues approaches risc architectures already defined bit extension tools analyzing use large address spaces several interesting methodology issues arise machine independent binaries risc instruction sets look almost identical sure cannot compile one program perhaps middle ground brand x generic risc could easily mapped variety isa reasonable efficiency one design developed pretty far osf raises another interesting question machine independent pipeline scheduling powerpc vs x architecture compare intel p processor ppc ppc dig technical information try come terms real differences p risc processor heart fuzz old x code really matter point project go trade rags beleive marketing hype long boosts sales validation cache studies using tools like spy spix piping trace directly optimized cache simulator possible evaluate substantial workloads billions instructions review literature see whether important published results cache studies really hold particular limit spec based caching studies point spec programs fit cache superscalar superpipeline high bandwidth data cache superscalar architectures take services several hits per cycle continue servicing hits misses processes next bigger slower level memory hierarchy much buy try sample design get feeling complexity build simple simulator run memory traces see much performance improvement possible sophisticated branching superscalar machines suggested steve krueger risc processors pipeline break relatively costly superscalar risc processors cost effectively multiplied number instructions execute simultaneously therefore desirable increase runs instructions without branches old architectures skip instructions used extensively seems skip instructions possibly restrictions could skipped could give conditional execution without breaking pipeline use squash kill hardware already place due needs exception processing study whether number cases skip instructions could effectively used great enough make useful skip n forms might extend usefulness allowing one instruction skipped several variants idea could considered including multiway branches conditional moves operators avoid branching e g max min abs conditional operators register organization superscalar machines superscalar execution looks attractive multiplier floating point units fit chip since cpi sit mostly idle however cost increase comes memory system see register file execute four instructions per cycle need port register file many values forwarded functional units never touch register file study register usage patterns superscalar designs see number ports reduced cost performance trade offs multiprocessors multiple processors chip allow transistor budget million transistors chip plenty room innovative designs multiple processors chip raises host interesting questions caches shared dedicated individual processors floating point units shared dedicated trade simpler processors vs fewer sophisticated ones sense trading instruction level parallelism process level parallelism serious bottleneck going pin bandwidth chip minimize bandwidth requirement new protocols required several ways frame studies context may want look multiple independent processes workstation many open windows small shared memory multiprocessor design may want look component massively parallel machine characterizing communication sharing multiprocessors current bus based multiprocessors interprocessor communication takes form cache misses thus several issues get folded single number miss rate good work done try characterize sharing terms modern cache organization remains many unanswered questions may answered inventing new analysis techniques certainly hard get useful data reference string generated processors depends work scheduled onto processors generally done allowing processors contend various scheduling data structures thus schedule somewhat dependent memory system exploring design variations relative fixed trace ignores feedback thorough study needs done sensitivity robustness multiprocessor address traces concerns raised number shared references available traces communication co processor using cache cache transfers several new multiprocessor designs use second processor network interface communicate main processor co processor using cache cache transfers results indicate actually slower one might first expect alternatives one suggestion opposite cache network interface processor buffers push data away right place memory system e g large blocks dram small things processor cache minimum cache miss rate due communication uniprocessor caches get larger miss rate approaches initial load cost compulsory miss rate would seem multiprocessor caches tend toward compulsory miss rate plus communication factor would optimal cache perform snoopy caches provide communication replication data replication causes coherence headaches miss rate communication rate decrease degree replication distributed shared memory atm distributed shared memory virtual shared memory around long time never caught large degree due fact ethernet slow make interesting atm change picture design implement dsm system atm network available dept apparently willi zwanepoole rice developed nice implementation could adapted networking following projects make use atm network department particular user level network interface ve developed deal experimental software willing hack around bugs gotchas ultra fast tcp ip implementation normal implementations tcp ip slow hell big part problem tcp ip normally sits kernel problems algorithms implementations implement really fast tcp user level atm carefully analyze performance werner vogels already prototype ready needs work connection set tear othewise already wuite fast done lot benchmarking particular study performance congestion done ultra fast rcp implementation similar review literature fast remote procedure call invocation implementations implement one atm cluster workstations atm multicast atm network supports multicast switch hardware figure control use interesting application e g horus maybe video broadcasting applications optimized application kernel implementation pick favorite compute intensive application beat hell compute intensive part example fastest mpeg encoder decoder build fastest motion detection algorithm video scene analysis fastest compression decompression key arrive fast implementation analysis convince others ve arrived best possible implementation truth specmarks prepare small suite benchmarks compare performance several workstations predicted specmarks would particularly interesting put together application specific suite example multimedia benchmark suite memory system behavior large systems study memory system behavior maybe processor performance parameters full fledged application isis comes mind measure path length reception message way lines code til reply sent back large system