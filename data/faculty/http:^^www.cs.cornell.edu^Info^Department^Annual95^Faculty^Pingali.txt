mime version server cern date wednesday nov gmt content type text html content length last modified thursday nov gmt keshav pingali keshav pingali associate professor phd mit research group works areas programming languages compilers parallel architectures goal develop tools generating parallel code applications programs deal large sparse matrices scientific applications involve numerical solution partial differential equations techniques used almost always produce system algebraic equations involve large sparse matrices unfortunately existing compiler technology poor job parallelizing sparse matrix programs take radically different approach problem compiler produces parallel sparse matrix programs sequential dense matrix programs using information user sparsity structure matrices program enables us use tools restructuring compiler area preliminary experiments krylov space solvers show code produced compiler competitive hand parallelized code libraries like argonne petsc library extend approach direct methods solving linear systems applications require adaptive mesh refinement project builds earlier work restructuring compilation techniques dense matrix programs developed restructuring techniques compiling programs distributed memory non uniform memory access numa architectures like ibm sp cm processor access local memory faster non local memory get good performance compiler must parallelize must also ensure locality reference matching code data distribution non local references must made block transfers preferable many small messages recently developed best algorithm known automatic alignment computation data incorporating compiler test bed earlier work developed novel loop restructuring technique called access normalization transforms loop nests increased locality potential block transfers implemented lambda loop transformation toolkit paper summarizing results won best paper prize asplos v worked hewlett packard transfer technology hp fortran compiler product line uniprocessors multiprocessors developed new frameworks program analysis optimization based dependence flow graph dfg dfg knits together data control dependence information program permitting development optimization algorithms generate better code possible competing approaches results independent interest example recently developed optimal algorithms control dependence problems answering foundational question open almost decade work led development linear time algorithm computing static single assignment ssa form programs results incorporated number compilers including ibm microsoft hp flavors professional activities panel member organizer acm symposium principles practice parallel programming member nsf national young investigator nyi awards panel consultant hewlett packard labs intel corporation army ballistic research labs odyssey research math sciences institute referee reviewer acm toplas ieee transactions computers journal parallel distributed computing journal supercomputing ieee computer editorial board international journal parallel programming awards national science foundation presidential young investigator ibm faculty development award best paper prize asplos v lectures fast algorithms control dependence problems hewlett packard corporation chelmsford massachusetts january computer science department wayne state university detroit michigan february rutgers university new brunswick new jersey may microsoft research laboratories redmond washington june publications solving alignment using elementary linear algebra proceedings seventh annual workshop languages compilers parallel computers lcpc lecture notes computer science ithaca ny august david bau induprakas kodukula vladimir kotlyar paul stodghill apt data structure optimal control dependence computation acm sigplan conference programming language design implementation pldi june gianfranco bilardi return annual report home page departmental home page questions comments please contact www cs cornell edu last modified november denise moore denise cs cornell edu