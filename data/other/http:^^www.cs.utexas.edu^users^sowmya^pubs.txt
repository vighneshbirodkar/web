mime version server cern date monday jan gmt content type text html content length last modified monday aug gmt papers view paper click open book image uncertain reasoning refinement bayesian networks combining connectionist symbolic techniques sowmya ramanchandran ph d proposal department computer sciences university texas austin bayesian networks provide mathematically sound formalism representing reasoning uncertain knowledge widely used however acquiring capturing knowledge framework difficult growing interest formulating techniques learning bayesian networks inductively problem learning bayesian network given complete data explored depth problem learning networks unobserved causes still open proposal view problem perspective theory revision present novel approach adapts techniques developed revising theories symbolic connectionist representations thus assume learner given initial approximate network usually obtained expert technique inductively revises network fit data better proposed system two components one component revises parameters bayesian network known structure component revises structure network component parameter revision maps given bayesian network multi layer feedforward neural network parameters mapped weights neural network uses standard backpropagation techniques learn weights structure revision component uses qualitative analysis suggest revisions network fails predict data accurately first component implemented present results experiments real world classification problems show technique effective also discuss proposed structure revision algorithm plans experiments evaluate system well extensions system revising bayesian network parameters using backpropagation sowmya ramachandran raymond j mooney appear proceedings international conference neural networks icnn washington d c june special session knowledge based artificial neural networks problem learning bayesian networks hidden variables known hard problem even simpler task learning conditional probabilities bayesian network hidden variables hard paper present approach learns conditional probabilities bayesian network hidden variables transforming multi layer feedforward neural network ann conditional probabilities mapped onto weights ann learned using standard backpropagation techniques avoid problem exponentially large anns focus bayesian networks noisy noisy nodes experiments real world classification problems demonstrate effectiveness technique qualitative modeling diagnosis learning qualitative models systems multiple operating regions sowmya ramachandran raymond j mooney benjamin j kuipers proceedings eight international workshop qualitative reasoning physical systems pp nara japan june qr problem learning qualitative models physical systems observations behaviour addressed several researchers recent years current techniques limit learning single qualitative differential equation model entire system however many systems several qualitative differential equations underlying paper present approach learning models systems technique divides behaviours segments explained single qualitative differential equation qualitative model segment generated using existing techniques learning single model show results applying technique several examples demonstrate effective neural networks information measure based skeletonisation sowmya ramachandran lorien y pratt advances neural information processing systems vol pp denver colorado automatic determination proper neural network topology trimming sized networks important area study previously addressed using variety techniues paper present information based skeletonisation imbs new approach problem superfluous hidden units removed based information measure im measure borrowed decision ttree induction techniques refelcts degree hyperplane formed hidden unit discriminates training data classes show results applying imbs three classification tasks demonstrate removes substantial number hidden units without significantly affecting network performance