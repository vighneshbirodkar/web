mime version server cern date sunday dec gmt content type text html content length last modified monday may gmt fine grain parallel cm rivl fine grain parallel cm rivl step towards real time multimedia processing jonathan barber barber cs cornell edu sugata mukhopadhyay sugata cs cornell edu cs final project professor thorsten von eicken department computer science cornell university table contents abstract introduction rivl generic parallel paradigm rivl graph parallelizing rivl continuous media parallel rivl implementations shared memory implementation networked implementation implementation caveats performance results extensions robustness conclusions references go back abstract form multimedia processing typically computationally expensive even harder problem performing form multimedia processing multiple real time continuous streams data paradigm frame sequence images incurs large computational expense obvious yet difficult solution divide problem compute solution parallel paper details nature problems solutions dealing parallel multimedia processing shared memory distributed environments click view slide show presentation paper go back introduction evolution rivl course past two years large effort mounted develop applications efficiently reliably process multimedia data effort manifested construction rivl resolution independent video language rivl multimedia processing package given set images set sequence images efficiently process multimedia streams generate outgoing image sequence images rivl implemented tcl extension capable performing common image operations overlay smoothing clipping cropping etc tcl interface simplifies process coding image processing script recently rivl extended process continuous streams multimedia data generate corresponding output stream images extended rivl package called cm rivl made possible treating rivl evaluation midpoint continuous media object work facilitated using cmt continuous media toolkit image processing continuous streams media real time hard problem considering today current state computer technology performing even simple image oper ation single sequence images outputting resultant image real time requires order million cpu cycles approach real time image processing frame rate frames per second standard frame rate perceiving continuous motion would require one following items true able perform image processing operations less linear time single processor able utilize high performance technology yet exist able divide work perform image processing parallel achieve less linear time performance since little control first two items focused efforts third image processing routines performed super linear time work divided among array parallel processors true rivl also cm rivl bearing mind established project goal develop easy use fast inexpensive real time multimedia processing application section describe generic method parallelizing image operations rivl exploiting way rivl processes inputted set images section describe two implementations parallel cm rivl privl first version designed run shared memory machines second version designed run cluster workstations section present analysis performance results section describe improvements implementations finally section draw conclusions analyze progress go back rivl generic parallel paradigm go back rivl graph begin discussion rivl introducing rivl evaluation graph order rivl execute requires set multimedia input data control rivl script rivl script sequence tcl rivl commands specify image processing operations occur input data rivl invoked rivl script translated rivl graph pictured node corresponds image operator e g im smooth im canny etc edge signal corresponds actual image data nodes lying inside illustrated rectangle correspond true image operators nodes lying outside rectangle rivl o nodes nodes outside left rectangle correspond read nodes e one read node per image stream node right rectangle corresponds write node want emphasize construction rivl graph compute multimedia data rivl graph merely control flow structure inputted sequence data must propagate generate outputted processed image two phases processing data using rivl graph constructed first phase manifests graph traversal right left makes rivl efficient image processing mechanism first node evaluated write node right node traversing graph reverse order rivl decides node exactly much data output signal requires input signal evaluation reverse propagated write node graph back every read node reverse propagation completes every node graph knows exactly much data input signal required compute node corresponding output signal multimedia data processed second traversal conforms left right traversal rivl graph propagating input data forwards graph operating data relevant final output image go back parallelizing rivl summarize preceding section statement amount data fetched read node exactly function output write node combining notion fact image processing operations rivl create dependencies one pixel another given input image derive simple mechanism dividing work parallelizing rivl instead running rivl single processor spawn multiple rivl processes different processors process work towards computing different segment output data define notion single master rivl process multiple slave rivl processes slave process started different processor started slave process sits idle listening instructions master process slave processes started master process created master process determines many slaves available work control connection established master every slave master assigns slave logical id master id slave id ranges n slaves slave assigned id master sends slave total number processes available work followed copy rivl script slave master receives rivl script generate copy rivl graph perform right left traversal independently difference right left traversal logical id current processor total number processes becomes factor determining much computation gets done process according figure amount data fetched read node longer function output write node function process logical id total number processes function write node output rivl process responsible computing different independent portion final output data based parameters hence term fine grain parallel cm rivl approach fine grained rivl process performs set computations different data actual data computation left right graph traversal occurs master says go slave master process computes appropriated portion output image go back continuous media parallel rivl model parallelization rivl described maps smoothly cm rivl cm rivl initial setup phase slave process master process previously described master process sends slave logical id total number processes copy rivl script rivl process computes rivl graph makes right left traversal image processing computing output frame continuous media stream occurs follows cmo continuous media object captures manages continuous streams data resides part master process cmo captured input data single output image contacts master parallel synchronization device tells rivl process slaves master data ready fetched computation begin asap rivl process fetches input data needs generate segment output data makes left right traversal graph output data rivl process written back buffer within cmo data re assembled single data output object rivl process blocks listening instructions cmo another image ready processing using method given stream multimedia data construction rivl graph reverse traversal graph performed setup time actual image processing requires one traversal graph rivl process computation area distributed among rivl processes go back implementations based generic parallelization scheme described preceding section developed two implementations parallel cm rivl implementation synchronization mechanism parallelizing independent rivl processes mechanism transferring data go back shared memory implementation shared memory implementation illustrated rivl process resides different processor processor resides machine access shared memory segment implementation mirrors generic parallel model described section implementation details initial setup facilitated using tcp ip multi cast via tcl dp process synchronization facilitated using unix semaphores data transfer facilitated using shared memory reads writes via unix ipc program compiled sparcstation running sunos model operates follows following initial setup phase cmo works capturing data necessary compute single rivl output frame cmo captures necessary data tells rivl process begin processing means entry semaphore rivl process reads data relevant output via shared memory read left right evaluation rivl graph completes rivl process performs shared memory write memory region containing output image accessible cmo rivl process blocks exit semaphore rivl processes complete computation frame data every rivl process blocks master rivl process un sets exit semaphore rivl process waits entry semaphore cmo releases go back networked implementation networked implementation illustrated rivl process resides different processor processor resides different machine implementation also mirrors generic parallel model described section implementation details initial setup facilitated using tcp ip multi cast via tcl dp data transfer facilitated using active messages u net synchronization mechanism implicit via active messages paradigm program compiled sparcstation running sunos model operates follows like shared memory counterpart model performs initial setup using ip multicast establish active message connections master slave rivl process cmo works capturing data necessary compute single rivl output frame model differs generic model master process knows exactly portion input data rivl process needs evaluate rivl graph cmo captures necessary data tells rivl process begin processing issuing gam store rivl process message received rivl process handler invoked tells rivl process begin evaluating rivl graph transferred data output data computed rivl process issues gam store master process specifying exactly sent data stored final output image buffer managed cmo eventually handler routine master process update received list master receives data rivl process cmo outputs computed frame begins processing next multimedia frame process synchronization mechanism implicit actual data transfer rivl process cannot begin evaluating graph given frame segment receives active message master process similarly master process cannot move next multimedia image receives active message slave process another subtle point master determine much input data rivl process requires rather rivl process determine information reduce round trip communication rate master slave rivl process compute region would require gam request followed gam reply master process instead master decides much data rivl process needs simply issues single gam store go back implementation caveats actual executables spmd separate executable master process another executable slave process didn cause problems developing shared memory implementation however since active messages ver assumes spmd model ran problems specifying handlers master process slave processes master process received active messages slave process slave process attempted invoke handler master existed slave handler situation slave process received active message master overcame shortcoming modifying active message source code modification allows application register handler active messages calling hid uam reg handler handler handler handler handler corresponds handler virtual address process returns hid integer stands handler id implementation since master executable slave executable different master slave must register handlers active message library process sends active message slave master vice versa longer ships processes virtual address handler rather ships logical id corresponding handler invoked active message library maintains look table indexed logical id logical id corresponds process handler virtual address invoked active messages go back performance results ran shared memory experiments quad processor sparcstation running sunos networked implementation tested using atm connected sparcstation running sunos constructed two different test cases named test test two tests perform following image operations test input sequences images first image sequence im scaled rotated copied four times resulting output overlayed onto second image sequence im output test input sequences images im scaled rotated copied four times im smoothed output im overlayed ontop output im overall test computationally expensive set operations test fact illustrated experimental results graphed results shared memory implementation performs somewhat better networked implementation implementations however perform better serial counterparts green bar graph one observation networked implementation exhibited large spread timings different frames attributed process getting preempted behavior visible shared memory implementation process sleeping waiting semaphores change process network implementation busy waits hopefully interrupt driven implementation active messages would cure note tests processor speed relatively equal results shared memory tests performance gains exhibited following patterns processors performance nearly doubled processors performance nearly doubled processors performance increase negligble performance increasing either communication overhead exceeds performance gain processors un optimally load balanced probably latter networked implementation processors performance nearly doubled processors small improvement performance however shared memory implementation appears little better processors performance increase negligble explanation probably shared memory experiment go back extensions robustness number improvements made improve overall performance robustness parallelization scheme improve load balance largest improvement involves improving load balance among rivl processes using hungry puppy strategy dividing work current implementations statically allocate work rivl process location amount data needed rivl process determined function number processes process id indicated experimental results significant boost rivl processes using shared memory implementation partly attribute problem un optimal load balance modifying networked implementation prove trouble improving overall load balance probably increase communication overhead active message sent processed modifying shared memory version easier current synchronization mechanism implemented using unix semaphores rivl process allowed begin executing next frame rivl processes completed execution current frame output image currently divided number processes available work could improve load balance implementation two things dividing output image work regions numerous smaller segments current frame allow rivl processes complete executing output segment grab another segment still need computed queue residing master process implementation improve load balance allowing less busy processes contribute equally entire output image giving busier processors time need compute data without becoming bottleneck entire output image improve reliability fault tolerance real time systems uncommon things go wrong specifically happen even slave rivl process crashes current implementations account mishaps process malfunction due either hardware communication failure implementation would fail port atm sparc implementations fast ethernet pc designing system cost always issue purpose implementing privl active messages utilize lower cost workstations networks compared expensive parallel machines cost higher performance pcs rapidly decline adapting implementations fast ethernet natural step reducing cost high performance cm privl actual transition atm sparc fast ethernet pc merely matter getting active messages work fast ethernet go back conclusions looking significant speedups parallel cm rivl moved n processors n results definitely encouraging shared memory implementation networked implementation obtained good speedups four processors order process real time data need approach frame processing rate close frames per second rougly ms per frame operations tested require upwards similar processors achieve desired frame rate results four processors however examining results determine current implementations processes running parallel cm rivl load balanced unfortunately must conclude implemenations stand scale upwards processors achieve desired frame rate however work way address load balancing problem furthermore hungry puppy object tracking algorithm currently incorporated privl experimental results available shortly however made significant progress parallelizing cm rivl cm rivl non trivial application parallelization scheme works standard rivl image operations go back references jonathan swartz brian c smith resolution independent video language proc third acm international conference multimedia san francisco ca november lawrence rowe brian c smith continuous media player third international workshop network operating systems support digital audio video nov san diego ca brian c smith lawrence rowe stephen c yen tcl distributed programming proc tcl tk workshop berkeley ca june von eicken d e culler c goldstein k e schauser active messages mechanism integrated communication computation proceedings th int l symp computer architecture may gold coast australia anindya basu vineet buch werner vogels thorsten von eicken u net user level network interface parallel distributed computing proc th acm symposium operating systems principles copper mountain colorado december sugata mukhopadhyay arun verma cmrivl programmable video gateway cornell university spring